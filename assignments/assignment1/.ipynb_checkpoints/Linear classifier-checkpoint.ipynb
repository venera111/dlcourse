{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Задание 1.2 - Линейный классификатор (Linear classifier)\n",
    "\n",
    "В этом задании мы реализуем другую модель машинного обучения - линейный классификатор. Линейный классификатор подбирает для каждого класса веса, на которые нужно умножить значение каждого признака и потом сложить вместе.\n",
    "Тот класс, у которого эта сумма больше, и является предсказанием модели.\n",
    "\n",
    "В этом задании вы:\n",
    "- потренируетесь считать градиенты различных многомерных функций\n",
    "- реализуете подсчет градиентов через линейную модель и функцию потерь softmax\n",
    "- реализуете процесс тренировки линейного классификатора\n",
    "- подберете параметры тренировки на практике\n",
    "\n",
    "На всякий случай, еще раз ссылка на туториал по numpy:  \n",
    "http://cs231n.github.io/python-numpy-tutorial/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataset import load_svhn, random_split_train_val\n",
    "from gradient_check import check_gradient\n",
    "from metrics import multiclass_accuracy\n",
    "import linear_classifer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Как всегда, первым делом загружаем данные\n",
    "\n",
    "Мы будем использовать все тот же SVHN."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_for_linear_classifier(train_X, test_X):\n",
    "    train_flat = train_X.reshape(train_X.shape[0], -1).astype(float) / 255.0\n",
    "    test_flat = test_X.reshape(test_X.shape[0], -1).astype(float) / 255.0\n",
    "    mean_image = np.mean(train_flat, axis=0)\n",
    "    \n",
    "    # Subtract mean\n",
    "    train_flat -= mean_image\n",
    "    test_flat -= mean_image\n",
    "\n",
    "    # Add another channel with ones as a bias term\n",
    "    train_flat_with_ones = np.hstack([train_flat, np.ones((train_X.shape[0], 1))])\n",
    "    test_flat_with_ones = np.hstack([test_flat, np.ones((test_X.shape[0], 1))])    \n",
    "    return train_flat_with_ones, test_flat_with_ones\n",
    "\n",
    "train_X, train_y, test_X, test_y = load_svhn(\"data\", max_train=10000, max_test=1000)    \n",
    "train_X, test_X = prepare_for_linear_classifier(train_X, test_X)\n",
    "# Split train into train and val\n",
    "train_X, train_y, val_X, val_y = random_split_train_val(train_X, train_y, num_val = 1000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Играемся с градиентами!\n",
    "\n",
    "В этом курсе мы будем писать много функций, которые вычисляют градиенты аналитическим методом.\n",
    "\n",
    "Все функции, в которых мы будем вычислять градиенты, будут написаны по одной и той же схеме.  \n",
    "Они будут получать на вход точку, где нужно вычислить значение и градиент функции, а на выходе будут выдавать кортеж (tuple) из двух значений - собственно значения функции в этой точке (всегда одно число) и аналитического значения градиента в той же точке (той же размерности, что и вход).\n",
    "```\n",
    "def f(x):\n",
    "    \"\"\"\n",
    "    Computes function and analytic gradient at x\n",
    "    \n",
    "    x: np array of float, input to the function\n",
    "    \n",
    "    Returns:\n",
    "    value: float, value of the function \n",
    "    grad: np array of float, same shape as x\n",
    "    \"\"\"\n",
    "    ...\n",
    "    \n",
    "    return value, grad\n",
    "```\n",
    "\n",
    "Необходимым инструментом во время реализации кода, вычисляющего градиенты, является функция его проверки. Эта функция вычисляет градиент численным методом и сверяет результат с градиентом, вычисленным аналитическим методом.\n",
    "\n",
    "Мы начнем с того, чтобы реализовать вычисление численного градиента (numeric gradient) в функции `check_gradient` в `gradient_check.py`. Эта функция будет принимать на вход функции формата, заданного выше, использовать значение `value` для вычисления численного градиента и сравнит его с аналитическим - они должны сходиться.\n",
    "\n",
    "Напишите часть функции, которая вычисляет градиент с помощью численной производной для каждой координаты. Для вычисления производной используйте так называемую two-point formula (https://en.wikipedia.org/wiki/Numerical_differentiation):\n",
    "\n",
    "![image](https://wikimedia.org/api/rest_v1/media/math/render/svg/22fc2c0a66c63560a349604f8b6b39221566236d)\n",
    "\n",
    "Все функции приведенные в следующей клетке должны проходить gradient check."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6.0 6.000000000039306\n",
      "Gradient check passed!\n",
      "1.0 0.9999999999621422\n",
      "1.0 0.9999999999621422\n",
      "Gradient check passed!\n",
      "1.0 0.9999999999621422\n",
      "1.0 0.9999999999621422\n",
      "1.0 0.9999999999621422\n",
      "1.0 0.9999999999621422\n",
      "Gradient check passed!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# TODO: Implement check_gradient function in gradient_check.py\n",
    "# All the functions below should pass the gradient check\n",
    "\n",
    "def square(x):\n",
    "    return float(x*x), 2*x\n",
    "\n",
    "check_gradient(square, np.array([3.0]))\n",
    "\n",
    "def array_sum(x):\n",
    "    assert x.shape == (2,), x.shape\n",
    "    return np.sum(x), np.ones_like(x)\n",
    "\n",
    "check_gradient(array_sum, np.array([3.0, 2.0]))\n",
    "\n",
    "def array_2d_sum(x):\n",
    "    assert x.shape == (2,2)\n",
    "    return np.sum(x), np.ones_like(x)\n",
    "\n",
    "check_gradient(array_2d_sum, np.array([[3.0, 2.0], [1.0, 0.0]]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Начинаем писать свои функции, считающие аналитический градиент\n",
    "\n",
    "Теперь реализуем функцию softmax, которая получает на вход оценки для каждого класса и преобразует их в вероятности от 0 до 1:\n",
    "![image](https://wikimedia.org/api/rest_v1/media/math/render/svg/e348290cf48ddbb6e9a6ef4e39363568b67c09d3)\n",
    "\n",
    "**Важно:** Практический аспект вычисления этой функции заключается в том, что в ней учавствует вычисление экспоненты от потенциально очень больших чисел - это может привести к очень большим значениям в числителе и знаменателе за пределами диапазона float.\n",
    "\n",
    "К счастью, у этой проблемы есть простое решение -- перед вычислением softmax вычесть из всех оценок максимальное значение среди всех оценок:\n",
    "```\n",
    "predictions -= np.max(predictions)\n",
    "```\n",
    "(подробнее здесь - http://cs231n.github.io/linear-classify/#softmax, секция `Practical issues: Numeric stability`)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO Implement softmax and cross-entropy for single sample\n",
    "probs = linear_classifer.softmax(np.array([-10, 0, 10]))\n",
    "\n",
    "# Make sure it works for big numbers too!\n",
    "probs = linear_classifer.softmax(np.array([1000, 0, 0]))\n",
    "assert np.isclose(probs[0], 1.0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Кроме этого, мы реализуем cross-entropy loss, которую мы будем использовать как функцию ошибки (error function).\n",
    "В общем виде cross-entropy определена следующим образом:\n",
    "![image](https://wikimedia.org/api/rest_v1/media/math/render/svg/0cb6da032ab424eefdca0884cd4113fe578f4293)\n",
    "\n",
    "где x - все классы, p(x) - истинная вероятность принадлежности сэмпла классу x, а q(x) - вероятность принадлежности классу x, предсказанная моделью.  \n",
    "В нашем случае сэмпл принадлежит только одному классу, индекс которого передается функции. Для него p(x) равна 1, а для остальных классов - 0. \n",
    "\n",
    "Это позволяет реализовать функцию проще!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5.006760443547122"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "probs = linear_classifer.softmax(np.array([-5, 0, 5]))\n",
    "linear_classifer.cross_entropy_loss(probs, 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "После того как мы реализовали сами функции, мы можем реализовать градиент.\n",
    "\n",
    "Оказывается, что вычисление градиента становится гораздо проще, если объединить эти функции в одну, которая сначала вычисляет вероятности через softmax, а потом использует их для вычисления функции ошибки через cross-entropy loss.\n",
    "\n",
    "Эта функция `softmax_with_cross_entropy` будет возвращает и значение ошибки, и градиент по входным параметрам. Мы проверим корректность реализации с помощью `check_gradient`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5761168847658291 0.5761168847651099\n",
      "-0.7880584423829146 -0.7880584423691771\n",
      "0.21194155761708544 0.2119415576151695\n",
      "Gradient check passed!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# TODO Implement combined function or softmax and cross entropy and produces gradient\n",
    "loss, grad = linear_classifer.softmax_with_cross_entropy(np.array([1, 0, 0]), 1)\n",
    "check_gradient(lambda x: linear_classifer.softmax_with_cross_entropy(x, 1), np.array([1, 0, 0], float))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "В качестве метода тренировки мы будем использовать стохастический градиентный спуск (stochastic gradient descent или SGD), который работает с батчами сэмплов. \n",
    "\n",
    "Поэтому все наши функции будут получать не один пример, а батч, то есть входом будет не вектор из `num_classes` оценок, а матрица размерности `batch_size, num_classes`. Индекс примера в батче всегда будет первым измерением.\n",
    "\n",
    "Следующий шаг - переписать наши функции так, чтобы они поддерживали батчи.\n",
    "\n",
    "Финальное значение функции ошибки должно остаться числом, и оно равно среднему значению ошибки среди всех примеров в батче."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.20603190919001857 0.20603190920009948\n",
      "0.5600527948339517 0.560052794829069\n",
      "-0.9721166132139888 -0.9721166132292679\n",
      "0.20603190919001857 0.20603190920009948\n",
      "Gradient check passed!\n",
      "0.2271508539361916 0.2271508539486433\n",
      "0.011309175094739847 0.011309175063090036\n",
      "0.011309175094739847 0.011309175063090036\n",
      "-0.24976920412567125 -0.24976920411923229\n",
      "0.03641059085767864 0.03641059085346399\n",
      "0.0989742474918849 0.0989742474866162\n",
      "0.0989742474918849 0.0989742474866162\n",
      "-0.23435908584144846 -0.23435908584890083\n",
      "0.05072100718053443 0.050721007172072057\n",
      "0.1378739921399875 0.13787399215647866\n",
      "-0.32646899146050945 -0.32646899146282493\n",
      "0.1378739921399875 0.13787399215647866\n",
      "Gradient check passed!\n"
     ]
    }
   ],
   "source": [
    "# TODO Extend combined function so it can receive a 2d array with batch of samples\n",
    "np.random.seed(42)\n",
    "# Test batch_size = 1\n",
    "num_classes = 4\n",
    "batch_size = 1\n",
    "predictions = np.random.randint(-1, 3, size=(batch_size, num_classes)).astype(float)\n",
    "target_index = np.random.randint(0, num_classes, size=(batch_size, 1)).astype(int)\n",
    "check_gradient(lambda x: linear_classifer.softmax_with_cross_entropy(x, target_index), predictions)\n",
    "\n",
    "# Test batch_size = 3\n",
    "num_classes = 4\n",
    "batch_size = 3\n",
    "predictions = np.random.randint(-1, 3, size=(batch_size, num_classes)).astype(float)\n",
    "target_index = np.random.randint(0, num_classes, size=(batch_size, 1)).astype(int)\n",
    "check_gradient(lambda x: linear_classifer.softmax_with_cross_entropy(x, target_index), predictions)\n",
    "\n",
    "# Make sure maximum subtraction for numberic stability is done separately for every sample in the batch\n",
    "probs = linear_classifer.softmax(np.array([[20,0,0], [1000, 0, 0]]))\n",
    "assert np.all(np.isclose(probs[:, 0], 1.0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Наконец, реализуем сам линейный классификатор!\n",
    "\n",
    "softmax и cross-entropy получают на вход оценки, которые выдает линейный классификатор.\n",
    "\n",
    "Он делает это очень просто: для каждого класса есть набор весов, на которые надо умножить пиксели картинки и сложить. Получившееся число и является оценкой класса, идущей на вход softmax.\n",
    "\n",
    "Таким образом, линейный классификатор можно представить как умножение вектора с пикселями на матрицу W размера `num_features, num_classes`. Такой подход легко расширяется на случай батча векторов с пикселями X размера `batch_size, num_features`:\n",
    "\n",
    "`predictions = X * W`, где `*` - матричное умножение.\n",
    "\n",
    "Реализуйте функцию подсчета линейного классификатора и градиентов по весам `linear_softmax` в файле `linear_classifer.py`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-0.44039853898894116 -0.4403985389922482\n",
      "0.4403985389889412 0.4403985389922482\n",
      "-0.4166856024001578 -0.4166856024112597\n",
      "0.4166856024001579 0.4166856024112597\n",
      "0.46411147557772453 0.4641114755732367\n",
      "-0.46411147557772453 -0.4641114755732367\n",
      "Gradient check passed!\n",
      "loss: 1.0877576813083574\n"
     ]
    }
   ],
   "source": [
    "# TODO Implement linear_softmax function that uses softmax with cross-entropy for linear classifier\n",
    "batch_size = 2\n",
    "num_classes = 2\n",
    "num_features = 3\n",
    "np.random.seed(42)\n",
    "W = np.random.randint(-1, 3, size=(num_features, num_classes)).astype(float)\n",
    "X = np.random.randint(-1, 3, size=(batch_size, num_features)).astype(float)\n",
    "target_index = np.ones(batch_size, dtype=int)\n",
    "loss, dW = linear_classifer.linear_softmax(X, W, target_index)\n",
    "check_gradient(lambda w: linear_classifer.linear_softmax(X, w, target_index), W)\n",
    "print(\"loss:\", loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### И теперь регуляризация\n",
    "\n",
    "Мы будем использовать L2 regularization для весов как часть общей функции ошибки.\n",
    "\n",
    "Напомним, L2 regularization определяется как\n",
    "\n",
    "l2_reg_loss = regularization_strength * sum<sub>ij</sub> W[i, j]<sup>2</sup>\n",
    "\n",
    "Реализуйте функцию для его вычисления и вычисления соотвествующих градиентов."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.02 0.020000000000575113\n",
      "0.04 0.03999999999976245\n",
      "-0.02 -0.020000000000575113\n",
      "0.02 0.020000000000575113\n",
      "0.02 0.020000000000575113\n",
      "0.04 0.03999999999976245\n",
      "Gradient check passed!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# TODO Implement l2_regularization function that implements loss for L2 regularization\n",
    "linear_classifer.l2_regularization(W, 0.01)\n",
    "check_gradient(lambda w: linear_classifer.l2_regularization(w, 0.01), W)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Тренировка!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Градиенты в порядке, реализуем процесс тренировки!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0, loss: 0.008018\n",
      "Epoch 1, loss: 0.007794\n",
      "Epoch 2, loss: 0.007729\n",
      "Epoch 3, loss: 0.007705\n",
      "Epoch 4, loss: 0.007703\n",
      "Epoch 5, loss: 0.007702\n",
      "Epoch 6, loss: 0.007701\n",
      "Epoch 7, loss: 0.007698\n",
      "Epoch 8, loss: 0.007696\n",
      "Epoch 9, loss: 0.007697\n"
     ]
    }
   ],
   "source": [
    "# TODO: Implement LinearSoftmaxClassifier.fit function\n",
    "classifier = linear_classifer.LinearSoftmaxClassifier()\n",
    "loss_history = classifier.fit(train_X, train_y, epochs=10, learning_rate=1e-3, batch_size=300, reg=1e1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# let's look at the loss history!\n",
    "plt.plot(loss_history);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy:  0.127\n",
      "Epoch 0, loss: 0.007698\n",
      "Epoch 1, loss: 0.007701\n",
      "Epoch 2, loss: 0.007700\n",
      "Epoch 3, loss: 0.007703\n",
      "Epoch 4, loss: 0.007699\n",
      "Epoch 5, loss: 0.007697\n",
      "Epoch 6, loss: 0.007696\n",
      "Epoch 7, loss: 0.007697\n",
      "Epoch 8, loss: 0.007700\n",
      "Epoch 9, loss: 0.007700\n",
      "Epoch 10, loss: 0.007698\n",
      "Epoch 11, loss: 0.007699\n",
      "Epoch 12, loss: 0.007696\n",
      "Epoch 13, loss: 0.007698\n",
      "Epoch 14, loss: 0.007700\n",
      "Epoch 15, loss: 0.007700\n",
      "Epoch 16, loss: 0.007696\n",
      "Epoch 17, loss: 0.007699\n",
      "Epoch 18, loss: 0.007700\n",
      "Epoch 19, loss: 0.007700\n",
      "Epoch 20, loss: 0.007701\n",
      "Epoch 21, loss: 0.007699\n",
      "Epoch 22, loss: 0.007700\n",
      "Epoch 23, loss: 0.007700\n",
      "Epoch 24, loss: 0.007700\n",
      "Epoch 25, loss: 0.007698\n",
      "Epoch 26, loss: 0.007697\n",
      "Epoch 27, loss: 0.007701\n",
      "Epoch 28, loss: 0.007698\n",
      "Epoch 29, loss: 0.007698\n",
      "Epoch 30, loss: 0.007697\n",
      "Epoch 31, loss: 0.007697\n",
      "Epoch 32, loss: 0.007698\n",
      "Epoch 33, loss: 0.007699\n",
      "Epoch 34, loss: 0.007699\n",
      "Epoch 35, loss: 0.007698\n",
      "Epoch 36, loss: 0.007701\n",
      "Epoch 37, loss: 0.007702\n",
      "Epoch 38, loss: 0.007697\n",
      "Epoch 39, loss: 0.007697\n",
      "Epoch 40, loss: 0.007698\n",
      "Epoch 41, loss: 0.007700\n",
      "Epoch 42, loss: 0.007700\n",
      "Epoch 43, loss: 0.007700\n",
      "Epoch 44, loss: 0.007701\n",
      "Epoch 45, loss: 0.007699\n",
      "Epoch 46, loss: 0.007698\n",
      "Epoch 47, loss: 0.007699\n",
      "Epoch 48, loss: 0.007700\n",
      "Epoch 49, loss: 0.007699\n",
      "Epoch 50, loss: 0.007699\n",
      "Epoch 51, loss: 0.007701\n",
      "Epoch 52, loss: 0.007699\n",
      "Epoch 53, loss: 0.007699\n",
      "Epoch 54, loss: 0.007698\n",
      "Epoch 55, loss: 0.007697\n",
      "Epoch 56, loss: 0.007700\n",
      "Epoch 57, loss: 0.007698\n",
      "Epoch 58, loss: 0.007699\n",
      "Epoch 59, loss: 0.007700\n",
      "Epoch 60, loss: 0.007697\n",
      "Epoch 61, loss: 0.007701\n",
      "Epoch 62, loss: 0.007699\n",
      "Epoch 63, loss: 0.007697\n",
      "Epoch 64, loss: 0.007700\n",
      "Epoch 65, loss: 0.007701\n",
      "Epoch 66, loss: 0.007700\n",
      "Epoch 67, loss: 0.007702\n",
      "Epoch 68, loss: 0.007697\n",
      "Epoch 69, loss: 0.007698\n",
      "Epoch 70, loss: 0.007697\n",
      "Epoch 71, loss: 0.007702\n",
      "Epoch 72, loss: 0.007697\n",
      "Epoch 73, loss: 0.007696\n",
      "Epoch 74, loss: 0.007700\n",
      "Epoch 75, loss: 0.007699\n",
      "Epoch 76, loss: 0.007698\n",
      "Epoch 77, loss: 0.007701\n",
      "Epoch 78, loss: 0.007700\n",
      "Epoch 79, loss: 0.007697\n",
      "Epoch 80, loss: 0.007699\n",
      "Epoch 81, loss: 0.007700\n",
      "Epoch 82, loss: 0.007696\n",
      "Epoch 83, loss: 0.007702\n",
      "Epoch 84, loss: 0.007699\n",
      "Epoch 85, loss: 0.007702\n",
      "Epoch 86, loss: 0.007699\n",
      "Epoch 87, loss: 0.007700\n",
      "Epoch 88, loss: 0.007699\n",
      "Epoch 89, loss: 0.007700\n",
      "Epoch 90, loss: 0.007697\n",
      "Epoch 91, loss: 0.007701\n",
      "Epoch 92, loss: 0.007699\n",
      "Epoch 93, loss: 0.007695\n",
      "Epoch 94, loss: 0.007700\n",
      "Epoch 95, loss: 0.007698\n",
      "Epoch 96, loss: 0.007701\n",
      "Epoch 97, loss: 0.007698\n",
      "Epoch 98, loss: 0.007701\n",
      "Epoch 99, loss: 0.007700\n",
      "Accuracy after training for 100 epochs:  0.121\n"
     ]
    }
   ],
   "source": [
    "# Let's check how it performs on validation set\n",
    "pred = classifier.predict(val_X)\n",
    "accuracy = multiclass_accuracy(pred, val_y)\n",
    "print(\"Accuracy: \", accuracy)\n",
    "\n",
    "# Now, let's train more and see if it performs better\n",
    "classifier.fit(train_X, train_y, epochs=100, learning_rate=1e-3, batch_size=300, reg=1e1)\n",
    "pred = classifier.predict(val_X)\n",
    "accuracy = multiclass_accuracy(pred, val_y)\n",
    "print(\"Accuracy after training for 100 epochs: \", accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Как и раньше, используем кросс-валидацию для подбора гиперпараметтов.\n",
    "\n",
    "В этот раз, чтобы тренировка занимала разумное время, мы будем использовать только одно разделение на тренировочные (training) и проверочные (validation) данные.\n",
    "\n",
    "Теперь нам нужно подобрать не один, а два гиперпараметра! Не ограничивайте себя изначальными значениями в коде.  \n",
    "Добейтесь точности более чем **20%** на проверочных данных (validation data)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0, loss: 0.007691\n",
      "Epoch 1, loss: 0.007691\n",
      "Epoch 2, loss: 0.007688\n",
      "Epoch 3, loss: 0.007686\n",
      "Epoch 4, loss: 0.007692\n",
      "Epoch 5, loss: 0.007684\n",
      "Epoch 6, loss: 0.007686\n",
      "Epoch 7, loss: 0.007680\n",
      "Epoch 8, loss: 0.007666\n",
      "Epoch 9, loss: 0.007667\n",
      "Epoch 10, loss: 0.007672\n",
      "Epoch 11, loss: 0.007667\n",
      "Epoch 12, loss: 0.007671\n",
      "Epoch 13, loss: 0.007638\n",
      "Epoch 14, loss: 0.007635\n",
      "Epoch 15, loss: 0.007644\n",
      "Epoch 16, loss: 0.007652\n",
      "Epoch 17, loss: 0.007645\n",
      "Epoch 18, loss: 0.007643\n",
      "Epoch 19, loss: 0.007644\n",
      "Epoch 20, loss: 0.007658\n",
      "Epoch 21, loss: 0.007636\n",
      "Epoch 22, loss: 0.007633\n",
      "Epoch 23, loss: 0.007634\n",
      "Epoch 24, loss: 0.007613\n",
      "Epoch 25, loss: 0.007635\n",
      "Epoch 26, loss: 0.007615\n",
      "Epoch 27, loss: 0.007633\n",
      "Epoch 28, loss: 0.007644\n",
      "Epoch 29, loss: 0.007628\n",
      "Epoch 30, loss: 0.007601\n",
      "Epoch 31, loss: 0.007600\n",
      "Epoch 32, loss: 0.007619\n",
      "Epoch 33, loss: 0.007610\n",
      "Epoch 34, loss: 0.007605\n",
      "Epoch 35, loss: 0.007603\n",
      "Epoch 36, loss: 0.007576\n",
      "Epoch 37, loss: 0.007583\n",
      "Epoch 38, loss: 0.007576\n",
      "Epoch 39, loss: 0.007580\n",
      "Epoch 40, loss: 0.007597\n",
      "Epoch 41, loss: 0.007602\n",
      "Epoch 42, loss: 0.007584\n",
      "Epoch 43, loss: 0.007570\n",
      "Epoch 44, loss: 0.007571\n",
      "Epoch 45, loss: 0.007562\n",
      "Epoch 46, loss: 0.007556\n",
      "Epoch 47, loss: 0.007577\n",
      "Epoch 48, loss: 0.007562\n",
      "Epoch 49, loss: 0.007578\n",
      "Epoch 50, loss: 0.007587\n",
      "Epoch 51, loss: 0.007569\n",
      "Epoch 52, loss: 0.007612\n",
      "Epoch 53, loss: 0.007546\n",
      "Epoch 54, loss: 0.007580\n",
      "Epoch 55, loss: 0.007557\n",
      "Epoch 56, loss: 0.007568\n",
      "Epoch 57, loss: 0.007528\n",
      "Epoch 58, loss: 0.007531\n",
      "Epoch 59, loss: 0.007544\n",
      "Epoch 60, loss: 0.007580\n",
      "Epoch 61, loss: 0.007547\n",
      "Epoch 62, loss: 0.007542\n",
      "Epoch 63, loss: 0.007548\n",
      "Epoch 64, loss: 0.007542\n",
      "Epoch 65, loss: 0.007559\n",
      "Epoch 66, loss: 0.007507\n",
      "Epoch 67, loss: 0.007522\n",
      "Epoch 68, loss: 0.007532\n",
      "Epoch 69, loss: 0.007515\n",
      "Epoch 70, loss: 0.007561\n",
      "Epoch 71, loss: 0.007585\n",
      "Epoch 72, loss: 0.007556\n",
      "Epoch 73, loss: 0.007483\n",
      "Epoch 74, loss: 0.007538\n",
      "Epoch 75, loss: 0.007479\n",
      "Epoch 76, loss: 0.007528\n",
      "Epoch 77, loss: 0.007570\n",
      "Epoch 78, loss: 0.007536\n",
      "Epoch 79, loss: 0.007552\n",
      "Epoch 80, loss: 0.007544\n",
      "Epoch 81, loss: 0.007493\n",
      "Epoch 82, loss: 0.007494\n",
      "Epoch 83, loss: 0.007561\n",
      "Epoch 84, loss: 0.007511\n",
      "Epoch 85, loss: 0.007541\n",
      "Epoch 86, loss: 0.007456\n",
      "Epoch 87, loss: 0.007540\n",
      "Epoch 88, loss: 0.007464\n",
      "Epoch 89, loss: 0.007499\n",
      "Epoch 90, loss: 0.007512\n",
      "Epoch 91, loss: 0.007493\n",
      "Epoch 92, loss: 0.007504\n",
      "Epoch 93, loss: 0.007452\n",
      "Epoch 94, loss: 0.007544\n",
      "Epoch 95, loss: 0.007431\n",
      "Epoch 96, loss: 0.007480\n",
      "Epoch 97, loss: 0.007496\n",
      "Epoch 98, loss: 0.007471\n",
      "Epoch 99, loss: 0.007489\n",
      "Epoch 100, loss: 0.007421\n",
      "Epoch 101, loss: 0.007472\n",
      "Epoch 102, loss: 0.007471\n",
      "Epoch 103, loss: 0.007445\n",
      "Epoch 104, loss: 0.007451\n",
      "Epoch 105, loss: 0.007418\n",
      "Epoch 106, loss: 0.007512\n",
      "Epoch 107, loss: 0.007390\n",
      "Epoch 108, loss: 0.007434\n",
      "Epoch 109, loss: 0.007490\n",
      "Epoch 110, loss: 0.007509\n",
      "Epoch 111, loss: 0.007483\n",
      "Epoch 112, loss: 0.007476\n",
      "Epoch 113, loss: 0.007406\n",
      "Epoch 114, loss: 0.007494\n",
      "Epoch 115, loss: 0.007464\n",
      "Epoch 116, loss: 0.007456\n",
      "Epoch 117, loss: 0.007520\n",
      "Epoch 118, loss: 0.007520\n",
      "Epoch 119, loss: 0.007469\n",
      "Epoch 120, loss: 0.007394\n",
      "Epoch 121, loss: 0.007420\n",
      "Epoch 122, loss: 0.007430\n",
      "Epoch 123, loss: 0.007420\n",
      "Epoch 124, loss: 0.007427\n",
      "Epoch 125, loss: 0.007402\n",
      "Epoch 126, loss: 0.007464\n",
      "Epoch 127, loss: 0.007448\n",
      "Epoch 128, loss: 0.007486\n",
      "Epoch 129, loss: 0.007392\n",
      "Epoch 130, loss: 0.007472\n",
      "Epoch 131, loss: 0.007469\n",
      "Epoch 132, loss: 0.007479\n",
      "Epoch 133, loss: 0.007445\n",
      "Epoch 134, loss: 0.007406\n",
      "Epoch 135, loss: 0.007507\n",
      "Epoch 136, loss: 0.007478\n",
      "Epoch 137, loss: 0.007411\n",
      "Epoch 138, loss: 0.007434\n",
      "Epoch 139, loss: 0.007443\n",
      "Epoch 140, loss: 0.007482\n",
      "Epoch 141, loss: 0.007413\n",
      "Epoch 142, loss: 0.007453\n",
      "Epoch 143, loss: 0.007418\n",
      "Epoch 144, loss: 0.007447\n",
      "Epoch 145, loss: 0.007365\n",
      "Epoch 146, loss: 0.007373\n",
      "Epoch 147, loss: 0.007356\n",
      "Epoch 148, loss: 0.007377\n",
      "Epoch 149, loss: 0.007371\n",
      "Epoch 150, loss: 0.007435\n",
      "Epoch 151, loss: 0.007396\n",
      "Epoch 152, loss: 0.007388\n",
      "Epoch 153, loss: 0.007323\n",
      "Epoch 154, loss: 0.007422\n",
      "Epoch 155, loss: 0.007383\n",
      "Epoch 156, loss: 0.007442\n",
      "Epoch 157, loss: 0.007405\n",
      "Epoch 158, loss: 0.007387\n",
      "Epoch 159, loss: 0.007452\n",
      "Epoch 160, loss: 0.007420\n",
      "Epoch 161, loss: 0.007418\n",
      "Epoch 162, loss: 0.007439\n",
      "Epoch 163, loss: 0.007474\n",
      "Epoch 164, loss: 0.007370\n",
      "Epoch 165, loss: 0.007445\n",
      "Epoch 166, loss: 0.007403\n",
      "Epoch 167, loss: 0.007380\n",
      "Epoch 168, loss: 0.007391\n",
      "Epoch 169, loss: 0.007410\n",
      "Epoch 170, loss: 0.007441\n",
      "Epoch 171, loss: 0.007373\n",
      "Epoch 172, loss: 0.007382\n",
      "Epoch 173, loss: 0.007382\n",
      "Epoch 174, loss: 0.007404\n",
      "Epoch 175, loss: 0.007368\n",
      "Epoch 176, loss: 0.007464\n",
      "Epoch 177, loss: 0.007315\n",
      "Epoch 178, loss: 0.007425\n",
      "Epoch 179, loss: 0.007327\n",
      "Epoch 180, loss: 0.007363\n",
      "Epoch 181, loss: 0.007369\n",
      "Epoch 182, loss: 0.007416\n",
      "Epoch 183, loss: 0.007439\n",
      "Epoch 184, loss: 0.007305\n",
      "Epoch 185, loss: 0.007395\n",
      "Epoch 186, loss: 0.007465\n",
      "Epoch 187, loss: 0.007296\n",
      "Epoch 188, loss: 0.007430\n",
      "Epoch 189, loss: 0.007416\n",
      "Epoch 190, loss: 0.007367\n",
      "Epoch 191, loss: 0.007426\n",
      "Epoch 192, loss: 0.007343\n",
      "Epoch 193, loss: 0.007308\n",
      "Epoch 194, loss: 0.007445\n",
      "Epoch 195, loss: 0.007420\n",
      "Epoch 196, loss: 0.007389\n",
      "Epoch 197, loss: 0.007450\n",
      "Epoch 198, loss: 0.007461\n",
      "Epoch 199, loss: 0.007346\n",
      "Epoch 0, loss: 0.007354\n",
      "Epoch 1, loss: 0.007344\n",
      "Epoch 2, loss: 0.007373\n",
      "Epoch 3, loss: 0.007339\n",
      "Epoch 4, loss: 0.007457\n",
      "Epoch 5, loss: 0.007287\n",
      "Epoch 6, loss: 0.007356\n",
      "Epoch 7, loss: 0.007384\n",
      "Epoch 8, loss: 0.007394\n",
      "Epoch 9, loss: 0.007379\n",
      "Epoch 10, loss: 0.007263\n",
      "Epoch 11, loss: 0.007442\n",
      "Epoch 12, loss: 0.007295\n",
      "Epoch 13, loss: 0.007462\n",
      "Epoch 14, loss: 0.007337\n",
      "Epoch 15, loss: 0.007314\n",
      "Epoch 16, loss: 0.007237\n",
      "Epoch 17, loss: 0.007369\n",
      "Epoch 18, loss: 0.007343\n",
      "Epoch 19, loss: 0.007325\n",
      "Epoch 20, loss: 0.007339\n",
      "Epoch 21, loss: 0.007360\n",
      "Epoch 22, loss: 0.007391\n",
      "Epoch 23, loss: 0.007339\n",
      "Epoch 24, loss: 0.007252\n",
      "Epoch 25, loss: 0.007367\n",
      "Epoch 26, loss: 0.007420\n",
      "Epoch 27, loss: 0.007298\n",
      "Epoch 28, loss: 0.007362\n",
      "Epoch 29, loss: 0.007355\n",
      "Epoch 30, loss: 0.007255\n",
      "Epoch 31, loss: 0.007333\n",
      "Epoch 32, loss: 0.007366\n",
      "Epoch 33, loss: 0.007358\n",
      "Epoch 34, loss: 0.007403\n",
      "Epoch 35, loss: 0.007275\n",
      "Epoch 36, loss: 0.007404\n",
      "Epoch 37, loss: 0.007275\n",
      "Epoch 38, loss: 0.007368\n",
      "Epoch 39, loss: 0.007369\n",
      "Epoch 40, loss: 0.007328\n",
      "Epoch 41, loss: 0.007285\n",
      "Epoch 42, loss: 0.007416\n",
      "Epoch 43, loss: 0.007325\n",
      "Epoch 44, loss: 0.007288\n",
      "Epoch 45, loss: 0.007246\n",
      "Epoch 46, loss: 0.007282\n",
      "Epoch 47, loss: 0.007383\n",
      "Epoch 48, loss: 0.007290\n",
      "Epoch 49, loss: 0.007328\n",
      "Epoch 50, loss: 0.007312\n",
      "Epoch 51, loss: 0.007345\n",
      "Epoch 52, loss: 0.007248\n",
      "Epoch 53, loss: 0.007319\n",
      "Epoch 54, loss: 0.007358\n",
      "Epoch 55, loss: 0.007248\n",
      "Epoch 56, loss: 0.007358\n",
      "Epoch 57, loss: 0.007306\n",
      "Epoch 58, loss: 0.007335\n",
      "Epoch 59, loss: 0.007368\n",
      "Epoch 60, loss: 0.007195\n",
      "Epoch 61, loss: 0.007403\n",
      "Epoch 62, loss: 0.007333\n",
      "Epoch 63, loss: 0.007233\n",
      "Epoch 64, loss: 0.007243\n",
      "Epoch 65, loss: 0.007437\n",
      "Epoch 66, loss: 0.007169\n",
      "Epoch 67, loss: 0.007240\n",
      "Epoch 68, loss: 0.007281\n",
      "Epoch 69, loss: 0.007294\n",
      "Epoch 70, loss: 0.007255\n",
      "Epoch 71, loss: 0.007264\n",
      "Epoch 72, loss: 0.007239\n",
      "Epoch 73, loss: 0.007358\n",
      "Epoch 74, loss: 0.007211\n",
      "Epoch 75, loss: 0.007329\n",
      "Epoch 76, loss: 0.007230\n",
      "Epoch 77, loss: 0.007399\n",
      "Epoch 78, loss: 0.007287\n",
      "Epoch 79, loss: 0.007306\n",
      "Epoch 80, loss: 0.007431\n",
      "Epoch 81, loss: 0.007292\n",
      "Epoch 82, loss: 0.007351\n",
      "Epoch 83, loss: 0.007266\n",
      "Epoch 84, loss: 0.007194\n",
      "Epoch 85, loss: 0.007360\n",
      "Epoch 86, loss: 0.007360\n",
      "Epoch 87, loss: 0.007353\n",
      "Epoch 88, loss: 0.007284\n",
      "Epoch 89, loss: 0.007247\n",
      "Epoch 90, loss: 0.007306\n",
      "Epoch 91, loss: 0.007163\n",
      "Epoch 92, loss: 0.007318\n",
      "Epoch 93, loss: 0.007250\n",
      "Epoch 94, loss: 0.007294\n",
      "Epoch 95, loss: 0.007373\n",
      "Epoch 96, loss: 0.007386\n",
      "Epoch 97, loss: 0.007191\n",
      "Epoch 98, loss: 0.007406\n",
      "Epoch 99, loss: 0.007384\n",
      "Epoch 100, loss: 0.007281\n",
      "Epoch 101, loss: 0.007331\n",
      "Epoch 102, loss: 0.007170\n",
      "Epoch 103, loss: 0.007271\n",
      "Epoch 104, loss: 0.007216\n",
      "Epoch 105, loss: 0.007319\n",
      "Epoch 106, loss: 0.007227\n",
      "Epoch 107, loss: 0.007245\n",
      "Epoch 108, loss: 0.007288\n",
      "Epoch 109, loss: 0.007304\n",
      "Epoch 110, loss: 0.007309\n",
      "Epoch 111, loss: 0.007273\n",
      "Epoch 112, loss: 0.007309\n",
      "Epoch 113, loss: 0.007303\n",
      "Epoch 114, loss: 0.007287\n",
      "Epoch 115, loss: 0.007328\n",
      "Epoch 116, loss: 0.007214\n",
      "Epoch 117, loss: 0.007270\n",
      "Epoch 118, loss: 0.007347\n",
      "Epoch 119, loss: 0.007350\n",
      "Epoch 120, loss: 0.007466\n",
      "Epoch 121, loss: 0.007525\n",
      "Epoch 122, loss: 0.007312\n",
      "Epoch 123, loss: 0.007231\n",
      "Epoch 124, loss: 0.007319\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 125, loss: 0.007417\n",
      "Epoch 126, loss: 0.007455\n",
      "Epoch 127, loss: 0.007167\n",
      "Epoch 128, loss: 0.007403\n",
      "Epoch 129, loss: 0.007319\n",
      "Epoch 130, loss: 0.007210\n",
      "Epoch 131, loss: 0.007278\n",
      "Epoch 132, loss: 0.007440\n",
      "Epoch 133, loss: 0.007213\n",
      "Epoch 134, loss: 0.007348\n",
      "Epoch 135, loss: 0.007262\n",
      "Epoch 136, loss: 0.007283\n",
      "Epoch 137, loss: 0.007348\n",
      "Epoch 138, loss: 0.007144\n",
      "Epoch 139, loss: 0.007267\n",
      "Epoch 140, loss: 0.007193\n",
      "Epoch 141, loss: 0.007350\n",
      "Epoch 142, loss: 0.007191\n",
      "Epoch 143, loss: 0.007232\n",
      "Epoch 144, loss: 0.007318\n",
      "Epoch 145, loss: 0.007312\n",
      "Epoch 146, loss: 0.007225\n",
      "Epoch 147, loss: 0.007331\n",
      "Epoch 148, loss: 0.007263\n",
      "Epoch 149, loss: 0.007183\n",
      "Epoch 150, loss: 0.007317\n",
      "Epoch 151, loss: 0.007194\n",
      "Epoch 152, loss: 0.007285\n",
      "Epoch 153, loss: 0.007280\n",
      "Epoch 154, loss: 0.007272\n",
      "Epoch 155, loss: 0.007329\n",
      "Epoch 156, loss: 0.007257\n",
      "Epoch 157, loss: 0.007240\n",
      "Epoch 158, loss: 0.007171\n",
      "Epoch 159, loss: 0.007146\n",
      "Epoch 160, loss: 0.007265\n",
      "Epoch 161, loss: 0.007307\n",
      "Epoch 162, loss: 0.007187\n",
      "Epoch 163, loss: 0.007302\n",
      "Epoch 164, loss: 0.007204\n",
      "Epoch 165, loss: 0.007290\n",
      "Epoch 166, loss: 0.007183\n",
      "Epoch 167, loss: 0.007336\n",
      "Epoch 168, loss: 0.007247\n",
      "Epoch 169, loss: 0.007264\n",
      "Epoch 170, loss: 0.007367\n",
      "Epoch 171, loss: 0.007189\n",
      "Epoch 172, loss: 0.007322\n",
      "Epoch 173, loss: 0.007267\n",
      "Epoch 174, loss: 0.007501\n",
      "Epoch 175, loss: 0.007112\n",
      "Epoch 176, loss: 0.007079\n",
      "Epoch 177, loss: 0.007301\n",
      "Epoch 178, loss: 0.007122\n",
      "Epoch 179, loss: 0.007237\n",
      "Epoch 180, loss: 0.007117\n",
      "Epoch 181, loss: 0.007267\n",
      "Epoch 182, loss: 0.007300\n",
      "Epoch 183, loss: 0.007258\n",
      "Epoch 184, loss: 0.007261\n",
      "Epoch 185, loss: 0.007282\n",
      "Epoch 186, loss: 0.007239\n",
      "Epoch 187, loss: 0.007319\n",
      "Epoch 188, loss: 0.007169\n",
      "Epoch 189, loss: 0.007221\n",
      "Epoch 190, loss: 0.007209\n",
      "Epoch 191, loss: 0.007278\n",
      "Epoch 192, loss: 0.007172\n",
      "Epoch 193, loss: 0.007214\n",
      "Epoch 194, loss: 0.007205\n",
      "Epoch 195, loss: 0.007292\n",
      "Epoch 196, loss: 0.007158\n",
      "Epoch 197, loss: 0.007325\n",
      "Epoch 198, loss: 0.007344\n",
      "Epoch 199, loss: 0.007238\n",
      "Epoch 0, loss: 0.007160\n",
      "Epoch 1, loss: 0.007251\n",
      "Epoch 2, loss: 0.007202\n",
      "Epoch 3, loss: 0.007098\n",
      "Epoch 4, loss: 0.007110\n",
      "Epoch 5, loss: 0.007197\n",
      "Epoch 6, loss: 0.007248\n",
      "Epoch 7, loss: 0.007234\n",
      "Epoch 8, loss: 0.007235\n",
      "Epoch 9, loss: 0.007245\n",
      "Epoch 10, loss: 0.007257\n",
      "Epoch 11, loss: 0.007277\n",
      "Epoch 12, loss: 0.007296\n",
      "Epoch 13, loss: 0.007277\n",
      "Epoch 14, loss: 0.007247\n",
      "Epoch 15, loss: 0.007170\n",
      "Epoch 16, loss: 0.007363\n",
      "Epoch 17, loss: 0.007269\n",
      "Epoch 18, loss: 0.007431\n",
      "Epoch 19, loss: 0.007379\n",
      "Epoch 20, loss: 0.007235\n",
      "Epoch 21, loss: 0.007126\n",
      "Epoch 22, loss: 0.007374\n",
      "Epoch 23, loss: 0.007174\n",
      "Epoch 24, loss: 0.007143\n",
      "Epoch 25, loss: 0.007284\n",
      "Epoch 26, loss: 0.007155\n",
      "Epoch 27, loss: 0.007107\n",
      "Epoch 28, loss: 0.007258\n",
      "Epoch 29, loss: 0.007249\n",
      "Epoch 30, loss: 0.007288\n",
      "Epoch 31, loss: 0.007335\n",
      "Epoch 32, loss: 0.007286\n",
      "Epoch 33, loss: 0.007170\n",
      "Epoch 34, loss: 0.007178\n",
      "Epoch 35, loss: 0.007119\n",
      "Epoch 36, loss: 0.007231\n",
      "Epoch 37, loss: 0.007109\n",
      "Epoch 38, loss: 0.007234\n",
      "Epoch 39, loss: 0.007223\n",
      "Epoch 40, loss: 0.007333\n",
      "Epoch 41, loss: 0.007329\n",
      "Epoch 42, loss: 0.007192\n",
      "Epoch 43, loss: 0.007196\n",
      "Epoch 44, loss: 0.007279\n",
      "Epoch 45, loss: 0.007272\n",
      "Epoch 46, loss: 0.007250\n",
      "Epoch 47, loss: 0.007274\n",
      "Epoch 48, loss: 0.007238\n",
      "Epoch 49, loss: 0.007032\n",
      "Epoch 50, loss: 0.007184\n",
      "Epoch 51, loss: 0.007181\n",
      "Epoch 52, loss: 0.007241\n",
      "Epoch 53, loss: 0.007038\n",
      "Epoch 54, loss: 0.007256\n",
      "Epoch 55, loss: 0.007304\n",
      "Epoch 56, loss: 0.007114\n",
      "Epoch 57, loss: 0.007119\n",
      "Epoch 58, loss: 0.007270\n",
      "Epoch 59, loss: 0.007211\n",
      "Epoch 60, loss: 0.007276\n",
      "Epoch 61, loss: 0.007209\n",
      "Epoch 62, loss: 0.007206\n",
      "Epoch 63, loss: 0.007179\n",
      "Epoch 64, loss: 0.007176\n",
      "Epoch 65, loss: 0.007167\n",
      "Epoch 66, loss: 0.007189\n",
      "Epoch 67, loss: 0.007128\n",
      "Epoch 68, loss: 0.007389\n",
      "Epoch 69, loss: 0.007289\n",
      "Epoch 70, loss: 0.007076\n",
      "Epoch 71, loss: 0.007233\n",
      "Epoch 72, loss: 0.007301\n",
      "Epoch 73, loss: 0.007255\n",
      "Epoch 74, loss: 0.007297\n",
      "Epoch 75, loss: 0.007128\n",
      "Epoch 76, loss: 0.007280\n",
      "Epoch 77, loss: 0.007289\n",
      "Epoch 78, loss: 0.007280\n",
      "Epoch 79, loss: 0.007150\n",
      "Epoch 80, loss: 0.007176\n",
      "Epoch 81, loss: 0.007164\n",
      "Epoch 82, loss: 0.007088\n",
      "Epoch 83, loss: 0.007143\n",
      "Epoch 84, loss: 0.007296\n",
      "Epoch 85, loss: 0.007376\n",
      "Epoch 86, loss: 0.007064\n",
      "Epoch 87, loss: 0.007236\n",
      "Epoch 88, loss: 0.007122\n",
      "Epoch 89, loss: 0.007154\n",
      "Epoch 90, loss: 0.007209\n",
      "Epoch 91, loss: 0.007155\n",
      "Epoch 92, loss: 0.007237\n",
      "Epoch 93, loss: 0.007174\n",
      "Epoch 94, loss: 0.007263\n",
      "Epoch 95, loss: 0.007056\n",
      "Epoch 96, loss: 0.007152\n",
      "Epoch 97, loss: 0.007382\n",
      "Epoch 98, loss: 0.007116\n",
      "Epoch 99, loss: 0.007263\n",
      "Epoch 100, loss: 0.006974\n",
      "Epoch 101, loss: 0.007164\n",
      "Epoch 102, loss: 0.007096\n",
      "Epoch 103, loss: 0.007206\n",
      "Epoch 104, loss: 0.007148\n",
      "Epoch 105, loss: 0.007051\n",
      "Epoch 106, loss: 0.007064\n",
      "Epoch 107, loss: 0.007282\n",
      "Epoch 108, loss: 0.007229\n",
      "Epoch 109, loss: 0.007247\n",
      "Epoch 110, loss: 0.007239\n",
      "Epoch 111, loss: 0.007237\n",
      "Epoch 112, loss: 0.007247\n",
      "Epoch 113, loss: 0.007194\n",
      "Epoch 114, loss: 0.007274\n",
      "Epoch 115, loss: 0.007351\n",
      "Epoch 116, loss: 0.007207\n",
      "Epoch 117, loss: 0.007243\n",
      "Epoch 118, loss: 0.007075\n",
      "Epoch 119, loss: 0.007247\n",
      "Epoch 120, loss: 0.007321\n",
      "Epoch 121, loss: 0.007131\n",
      "Epoch 122, loss: 0.007145\n",
      "Epoch 123, loss: 0.007270\n",
      "Epoch 124, loss: 0.007153\n",
      "Epoch 125, loss: 0.007126\n",
      "Epoch 126, loss: 0.007092\n",
      "Epoch 127, loss: 0.007174\n",
      "Epoch 128, loss: 0.007216\n",
      "Epoch 129, loss: 0.007180\n",
      "Epoch 130, loss: 0.007319\n",
      "Epoch 131, loss: 0.007144\n",
      "Epoch 132, loss: 0.007142\n",
      "Epoch 133, loss: 0.007316\n",
      "Epoch 134, loss: 0.007289\n",
      "Epoch 135, loss: 0.007222\n",
      "Epoch 136, loss: 0.007023\n",
      "Epoch 137, loss: 0.007203\n",
      "Epoch 138, loss: 0.007191\n",
      "Epoch 139, loss: 0.007053\n",
      "Epoch 140, loss: 0.007287\n",
      "Epoch 141, loss: 0.007202\n",
      "Epoch 142, loss: 0.007182\n",
      "Epoch 143, loss: 0.007181\n",
      "Epoch 144, loss: 0.007076\n",
      "Epoch 145, loss: 0.007171\n",
      "Epoch 146, loss: 0.007148\n",
      "Epoch 147, loss: 0.007186\n",
      "Epoch 148, loss: 0.007237\n",
      "Epoch 149, loss: 0.007131\n",
      "Epoch 150, loss: 0.007346\n",
      "Epoch 151, loss: 0.007280\n",
      "Epoch 152, loss: 0.007110\n",
      "Epoch 153, loss: 0.007319\n",
      "Epoch 154, loss: 0.007158\n",
      "Epoch 155, loss: 0.007124\n",
      "Epoch 156, loss: 0.007152\n",
      "Epoch 157, loss: 0.007428\n",
      "Epoch 158, loss: 0.007248\n",
      "Epoch 159, loss: 0.007241\n",
      "Epoch 160, loss: 0.007093\n",
      "Epoch 161, loss: 0.007118\n",
      "Epoch 162, loss: 0.007252\n",
      "Epoch 163, loss: 0.007163\n",
      "Epoch 164, loss: 0.007014\n",
      "Epoch 165, loss: 0.007202\n",
      "Epoch 166, loss: 0.007142\n",
      "Epoch 167, loss: 0.007170\n",
      "Epoch 168, loss: 0.007112\n",
      "Epoch 169, loss: 0.007266\n",
      "Epoch 170, loss: 0.007271\n",
      "Epoch 171, loss: 0.007244\n",
      "Epoch 172, loss: 0.007193\n",
      "Epoch 173, loss: 0.007147\n",
      "Epoch 174, loss: 0.007324\n",
      "Epoch 175, loss: 0.007235\n",
      "Epoch 176, loss: 0.007300\n",
      "Epoch 177, loss: 0.007248\n",
      "Epoch 178, loss: 0.007127\n",
      "Epoch 179, loss: 0.007399\n",
      "Epoch 180, loss: 0.007244\n",
      "Epoch 181, loss: 0.007012\n",
      "Epoch 182, loss: 0.007278\n",
      "Epoch 183, loss: 0.007117\n",
      "Epoch 184, loss: 0.007219\n",
      "Epoch 185, loss: 0.007128\n",
      "Epoch 186, loss: 0.007157\n",
      "Epoch 187, loss: 0.007163\n",
      "Epoch 188, loss: 0.007192\n",
      "Epoch 189, loss: 0.007263\n",
      "Epoch 190, loss: 0.007171\n",
      "Epoch 191, loss: 0.007259\n",
      "Epoch 192, loss: 0.007245\n",
      "Epoch 193, loss: 0.007135\n",
      "Epoch 194, loss: 0.007098\n",
      "Epoch 195, loss: 0.007252\n",
      "Epoch 196, loss: 0.007140\n",
      "Epoch 197, loss: 0.007185\n",
      "Epoch 198, loss: 0.007057\n",
      "Epoch 199, loss: 0.007300\n",
      "Epoch 0, loss: 0.007275\n",
      "Epoch 1, loss: 0.007135\n",
      "Epoch 2, loss: 0.007136\n",
      "Epoch 3, loss: 0.007241\n",
      "Epoch 4, loss: 0.007252\n",
      "Epoch 5, loss: 0.007221\n",
      "Epoch 6, loss: 0.007190\n",
      "Epoch 7, loss: 0.007203\n",
      "Epoch 8, loss: 0.007170\n",
      "Epoch 9, loss: 0.007181\n",
      "Epoch 10, loss: 0.007215\n",
      "Epoch 11, loss: 0.007249\n",
      "Epoch 12, loss: 0.007118\n",
      "Epoch 13, loss: 0.007087\n",
      "Epoch 14, loss: 0.007313\n",
      "Epoch 15, loss: 0.007289\n",
      "Epoch 16, loss: 0.007147\n",
      "Epoch 17, loss: 0.007325\n",
      "Epoch 18, loss: 0.007226\n",
      "Epoch 19, loss: 0.007145\n",
      "Epoch 20, loss: 0.007178\n",
      "Epoch 21, loss: 0.007262\n",
      "Epoch 22, loss: 0.007049\n",
      "Epoch 23, loss: 0.007119\n",
      "Epoch 24, loss: 0.007190\n",
      "Epoch 25, loss: 0.007202\n",
      "Epoch 26, loss: 0.006999\n",
      "Epoch 27, loss: 0.007278\n",
      "Epoch 28, loss: 0.007165\n",
      "Epoch 29, loss: 0.007115\n",
      "Epoch 30, loss: 0.007177\n",
      "Epoch 31, loss: 0.007014\n",
      "Epoch 32, loss: 0.007055\n",
      "Epoch 33, loss: 0.007209\n",
      "Epoch 34, loss: 0.007173\n",
      "Epoch 35, loss: 0.007231\n",
      "Epoch 36, loss: 0.007189\n",
      "Epoch 37, loss: 0.007221\n",
      "Epoch 38, loss: 0.007157\n",
      "Epoch 39, loss: 0.007080\n",
      "Epoch 40, loss: 0.007157\n",
      "Epoch 41, loss: 0.007214\n",
      "Epoch 42, loss: 0.007304\n",
      "Epoch 43, loss: 0.007172\n",
      "Epoch 44, loss: 0.007323\n",
      "Epoch 45, loss: 0.007203\n",
      "Epoch 46, loss: 0.007142\n",
      "Epoch 47, loss: 0.007277\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 48, loss: 0.007183\n",
      "Epoch 49, loss: 0.007149\n",
      "Epoch 50, loss: 0.007215\n",
      "Epoch 51, loss: 0.007221\n",
      "Epoch 52, loss: 0.007255\n",
      "Epoch 53, loss: 0.007165\n",
      "Epoch 54, loss: 0.007214\n",
      "Epoch 55, loss: 0.007140\n",
      "Epoch 56, loss: 0.007312\n",
      "Epoch 57, loss: 0.007287\n",
      "Epoch 58, loss: 0.007191\n",
      "Epoch 59, loss: 0.007388\n",
      "Epoch 60, loss: 0.007265\n",
      "Epoch 61, loss: 0.007129\n",
      "Epoch 62, loss: 0.007148\n",
      "Epoch 63, loss: 0.007342\n",
      "Epoch 64, loss: 0.006983\n",
      "Epoch 65, loss: 0.007144\n",
      "Epoch 66, loss: 0.007187\n",
      "Epoch 67, loss: 0.007250\n",
      "Epoch 68, loss: 0.007183\n",
      "Epoch 69, loss: 0.007184\n",
      "Epoch 70, loss: 0.007195\n",
      "Epoch 71, loss: 0.007121\n",
      "Epoch 72, loss: 0.007095\n",
      "Epoch 73, loss: 0.007123\n",
      "Epoch 74, loss: 0.007202\n",
      "Epoch 75, loss: 0.007202\n",
      "Epoch 76, loss: 0.007210\n",
      "Epoch 77, loss: 0.007118\n",
      "Epoch 78, loss: 0.007218\n",
      "Epoch 79, loss: 0.007074\n",
      "Epoch 80, loss: 0.007164\n",
      "Epoch 81, loss: 0.007183\n",
      "Epoch 82, loss: 0.007236\n",
      "Epoch 83, loss: 0.007202\n",
      "Epoch 84, loss: 0.007248\n",
      "Epoch 85, loss: 0.007127\n",
      "Epoch 86, loss: 0.007350\n",
      "Epoch 87, loss: 0.007332\n",
      "Epoch 88, loss: 0.007156\n",
      "Epoch 89, loss: 0.007146\n",
      "Epoch 90, loss: 0.007095\n",
      "Epoch 91, loss: 0.007047\n",
      "Epoch 92, loss: 0.007144\n",
      "Epoch 93, loss: 0.007225\n",
      "Epoch 94, loss: 0.007301\n",
      "Epoch 95, loss: 0.007218\n",
      "Epoch 96, loss: 0.007282\n",
      "Epoch 97, loss: 0.007173\n",
      "Epoch 98, loss: 0.007139\n",
      "Epoch 99, loss: 0.007092\n",
      "Epoch 100, loss: 0.007078\n",
      "Epoch 101, loss: 0.007094\n",
      "Epoch 102, loss: 0.007157\n",
      "Epoch 103, loss: 0.007052\n",
      "Epoch 104, loss: 0.007346\n",
      "Epoch 105, loss: 0.007353\n",
      "Epoch 106, loss: 0.007289\n",
      "Epoch 107, loss: 0.007280\n",
      "Epoch 108, loss: 0.007097\n",
      "Epoch 109, loss: 0.006992\n",
      "Epoch 110, loss: 0.007266\n",
      "Epoch 111, loss: 0.007437\n",
      "Epoch 112, loss: 0.007248\n",
      "Epoch 113, loss: 0.007122\n",
      "Epoch 114, loss: 0.007193\n",
      "Epoch 115, loss: 0.007128\n",
      "Epoch 116, loss: 0.007413\n",
      "Epoch 117, loss: 0.007189\n",
      "Epoch 118, loss: 0.007326\n",
      "Epoch 119, loss: 0.007306\n",
      "Epoch 120, loss: 0.007381\n",
      "Epoch 121, loss: 0.007170\n",
      "Epoch 122, loss: 0.007107\n",
      "Epoch 123, loss: 0.007134\n",
      "Epoch 124, loss: 0.007166\n",
      "Epoch 125, loss: 0.007286\n",
      "Epoch 126, loss: 0.007268\n",
      "Epoch 127, loss: 0.007290\n",
      "Epoch 128, loss: 0.007236\n",
      "Epoch 129, loss: 0.007102\n",
      "Epoch 130, loss: 0.007319\n",
      "Epoch 131, loss: 0.007308\n",
      "Epoch 132, loss: 0.007285\n",
      "Epoch 133, loss: 0.007052\n",
      "Epoch 134, loss: 0.007160\n",
      "Epoch 135, loss: 0.007001\n",
      "Epoch 136, loss: 0.007039\n",
      "Epoch 137, loss: 0.007210\n",
      "Epoch 138, loss: 0.007235\n",
      "Epoch 139, loss: 0.007158\n",
      "Epoch 140, loss: 0.007389\n",
      "Epoch 141, loss: 0.007039\n",
      "Epoch 142, loss: 0.007240\n",
      "Epoch 143, loss: 0.007147\n",
      "Epoch 144, loss: 0.007121\n",
      "Epoch 145, loss: 0.007058\n",
      "Epoch 146, loss: 0.007264\n",
      "Epoch 147, loss: 0.007102\n",
      "Epoch 148, loss: 0.007046\n",
      "Epoch 149, loss: 0.007132\n",
      "Epoch 150, loss: 0.007086\n",
      "Epoch 151, loss: 0.007144\n",
      "Epoch 152, loss: 0.007166\n",
      "Epoch 153, loss: 0.007299\n",
      "Epoch 154, loss: 0.007197\n",
      "Epoch 155, loss: 0.007121\n",
      "Epoch 156, loss: 0.007176\n",
      "Epoch 157, loss: 0.007288\n",
      "Epoch 158, loss: 0.007113\n",
      "Epoch 159, loss: 0.007231\n",
      "Epoch 160, loss: 0.007206\n",
      "Epoch 161, loss: 0.007240\n",
      "Epoch 162, loss: 0.007166\n",
      "Epoch 163, loss: 0.007008\n",
      "Epoch 164, loss: 0.007223\n",
      "Epoch 165, loss: 0.007020\n",
      "Epoch 166, loss: 0.007003\n",
      "Epoch 167, loss: 0.007034\n",
      "Epoch 168, loss: 0.007039\n",
      "Epoch 169, loss: 0.007186\n",
      "Epoch 170, loss: 0.007138\n",
      "Epoch 171, loss: 0.007143\n",
      "Epoch 172, loss: 0.007049\n",
      "Epoch 173, loss: 0.007198\n",
      "Epoch 174, loss: 0.007181\n",
      "Epoch 175, loss: 0.007231\n",
      "Epoch 176, loss: 0.007198\n",
      "Epoch 177, loss: 0.007225\n",
      "Epoch 178, loss: 0.007145\n",
      "Epoch 179, loss: 0.007205\n",
      "Epoch 180, loss: 0.007175\n",
      "Epoch 181, loss: 0.007060\n",
      "Epoch 182, loss: 0.007136\n",
      "Epoch 183, loss: 0.007150\n",
      "Epoch 184, loss: 0.007170\n",
      "Epoch 185, loss: 0.007263\n",
      "Epoch 186, loss: 0.007137\n",
      "Epoch 187, loss: 0.007206\n",
      "Epoch 188, loss: 0.007202\n",
      "Epoch 189, loss: 0.007231\n",
      "Epoch 190, loss: 0.007117\n",
      "Epoch 191, loss: 0.007251\n",
      "Epoch 192, loss: 0.007346\n",
      "Epoch 193, loss: 0.007302\n",
      "Epoch 194, loss: 0.007235\n",
      "Epoch 195, loss: 0.007160\n",
      "Epoch 196, loss: 0.007166\n",
      "Epoch 197, loss: 0.007066\n",
      "Epoch 198, loss: 0.007191\n",
      "Epoch 199, loss: 0.007140\n",
      "Epoch 0, loss: 0.007168\n",
      "Epoch 1, loss: 0.006935\n",
      "Epoch 2, loss: 0.007113\n",
      "Epoch 3, loss: 0.007065\n",
      "Epoch 4, loss: 0.007177\n",
      "Epoch 5, loss: 0.007297\n",
      "Epoch 6, loss: 0.007256\n",
      "Epoch 7, loss: 0.007145\n",
      "Epoch 8, loss: 0.007139\n",
      "Epoch 9, loss: 0.007140\n",
      "Epoch 10, loss: 0.007118\n",
      "Epoch 11, loss: 0.007262\n",
      "Epoch 12, loss: 0.007307\n",
      "Epoch 13, loss: 0.007162\n",
      "Epoch 14, loss: 0.007118\n",
      "Epoch 15, loss: 0.007208\n",
      "Epoch 16, loss: 0.007161\n",
      "Epoch 17, loss: 0.007043\n",
      "Epoch 18, loss: 0.007059\n",
      "Epoch 19, loss: 0.007166\n",
      "Epoch 20, loss: 0.007327\n",
      "Epoch 21, loss: 0.007191\n",
      "Epoch 22, loss: 0.007144\n",
      "Epoch 23, loss: 0.007155\n",
      "Epoch 24, loss: 0.007169\n",
      "Epoch 25, loss: 0.007308\n",
      "Epoch 26, loss: 0.007275\n",
      "Epoch 27, loss: 0.007141\n",
      "Epoch 28, loss: 0.007138\n",
      "Epoch 29, loss: 0.007182\n",
      "Epoch 30, loss: 0.007032\n",
      "Epoch 31, loss: 0.007345\n",
      "Epoch 32, loss: 0.007128\n",
      "Epoch 33, loss: 0.007113\n",
      "Epoch 34, loss: 0.007188\n",
      "Epoch 35, loss: 0.007102\n",
      "Epoch 36, loss: 0.007270\n",
      "Epoch 37, loss: 0.007253\n",
      "Epoch 38, loss: 0.007215\n",
      "Epoch 39, loss: 0.007246\n",
      "Epoch 40, loss: 0.007100\n",
      "Epoch 41, loss: 0.007134\n",
      "Epoch 42, loss: 0.007368\n",
      "Epoch 43, loss: 0.007123\n",
      "Epoch 44, loss: 0.007126\n",
      "Epoch 45, loss: 0.007170\n",
      "Epoch 46, loss: 0.007218\n",
      "Epoch 47, loss: 0.007198\n",
      "Epoch 48, loss: 0.007079\n",
      "Epoch 49, loss: 0.007071\n",
      "Epoch 50, loss: 0.007117\n",
      "Epoch 51, loss: 0.007177\n",
      "Epoch 52, loss: 0.007069\n",
      "Epoch 53, loss: 0.007195\n",
      "Epoch 54, loss: 0.006997\n",
      "Epoch 55, loss: 0.007103\n",
      "Epoch 56, loss: 0.007230\n",
      "Epoch 57, loss: 0.007153\n",
      "Epoch 58, loss: 0.007155\n",
      "Epoch 59, loss: 0.007080\n",
      "Epoch 60, loss: 0.007158\n",
      "Epoch 61, loss: 0.007197\n",
      "Epoch 62, loss: 0.007108\n",
      "Epoch 63, loss: 0.007236\n",
      "Epoch 64, loss: 0.007174\n",
      "Epoch 65, loss: 0.007210\n",
      "Epoch 66, loss: 0.007095\n",
      "Epoch 67, loss: 0.007072\n",
      "Epoch 68, loss: 0.007163\n",
      "Epoch 69, loss: 0.007226\n",
      "Epoch 70, loss: 0.007147\n",
      "Epoch 71, loss: 0.007288\n",
      "Epoch 72, loss: 0.007092\n",
      "Epoch 73, loss: 0.007029\n",
      "Epoch 74, loss: 0.007233\n",
      "Epoch 75, loss: 0.007154\n",
      "Epoch 76, loss: 0.007238\n",
      "Epoch 77, loss: 0.007183\n",
      "Epoch 78, loss: 0.007114\n",
      "Epoch 79, loss: 0.007218\n",
      "Epoch 80, loss: 0.007112\n",
      "Epoch 81, loss: 0.007014\n",
      "Epoch 82, loss: 0.007192\n",
      "Epoch 83, loss: 0.007188\n",
      "Epoch 84, loss: 0.006920\n",
      "Epoch 85, loss: 0.007176\n",
      "Epoch 86, loss: 0.007298\n",
      "Epoch 87, loss: 0.007211\n",
      "Epoch 88, loss: 0.007190\n",
      "Epoch 89, loss: 0.006982\n",
      "Epoch 90, loss: 0.007160\n",
      "Epoch 91, loss: 0.007126\n",
      "Epoch 92, loss: 0.007211\n",
      "Epoch 93, loss: 0.007292\n",
      "Epoch 94, loss: 0.007126\n",
      "Epoch 95, loss: 0.007230\n",
      "Epoch 96, loss: 0.007394\n",
      "Epoch 97, loss: 0.007217\n",
      "Epoch 98, loss: 0.007323\n",
      "Epoch 99, loss: 0.007201\n",
      "Epoch 100, loss: 0.007172\n",
      "Epoch 101, loss: 0.007126\n",
      "Epoch 102, loss: 0.007275\n",
      "Epoch 103, loss: 0.007172\n",
      "Epoch 104, loss: 0.007204\n",
      "Epoch 105, loss: 0.007112\n",
      "Epoch 106, loss: 0.007022\n",
      "Epoch 107, loss: 0.007240\n",
      "Epoch 108, loss: 0.007120\n",
      "Epoch 109, loss: 0.007162\n",
      "Epoch 110, loss: 0.007216\n",
      "Epoch 111, loss: 0.007309\n",
      "Epoch 112, loss: 0.007202\n",
      "Epoch 113, loss: 0.007324\n",
      "Epoch 114, loss: 0.007219\n",
      "Epoch 115, loss: 0.007083\n",
      "Epoch 116, loss: 0.007017\n",
      "Epoch 117, loss: 0.007153\n",
      "Epoch 118, loss: 0.007250\n",
      "Epoch 119, loss: 0.007152\n",
      "Epoch 120, loss: 0.007036\n",
      "Epoch 121, loss: 0.007098\n",
      "Epoch 122, loss: 0.007334\n",
      "Epoch 123, loss: 0.007239\n",
      "Epoch 124, loss: 0.007178\n",
      "Epoch 125, loss: 0.007185\n",
      "Epoch 126, loss: 0.007175\n",
      "Epoch 127, loss: 0.007290\n",
      "Epoch 128, loss: 0.007237\n",
      "Epoch 129, loss: 0.007346\n",
      "Epoch 130, loss: 0.007108\n",
      "Epoch 131, loss: 0.007143\n",
      "Epoch 132, loss: 0.007155\n",
      "Epoch 133, loss: 0.007121\n",
      "Epoch 134, loss: 0.007064\n",
      "Epoch 135, loss: 0.007208\n",
      "Epoch 136, loss: 0.007036\n",
      "Epoch 137, loss: 0.007243\n",
      "Epoch 138, loss: 0.007078\n",
      "Epoch 139, loss: 0.007071\n",
      "Epoch 140, loss: 0.007163\n",
      "Epoch 141, loss: 0.007187\n",
      "Epoch 142, loss: 0.007279\n",
      "Epoch 143, loss: 0.007245\n",
      "Epoch 144, loss: 0.007096\n",
      "Epoch 145, loss: 0.007200\n",
      "Epoch 146, loss: 0.007186\n",
      "Epoch 147, loss: 0.007186\n",
      "Epoch 148, loss: 0.007181\n",
      "Epoch 149, loss: 0.007063\n",
      "Epoch 150, loss: 0.007163\n",
      "Epoch 151, loss: 0.007166\n",
      "Epoch 152, loss: 0.007303\n",
      "Epoch 153, loss: 0.007071\n",
      "Epoch 154, loss: 0.007265\n",
      "Epoch 155, loss: 0.007114\n",
      "Epoch 156, loss: 0.007271\n",
      "Epoch 157, loss: 0.007109\n",
      "Epoch 158, loss: 0.007171\n",
      "Epoch 159, loss: 0.007374\n",
      "Epoch 160, loss: 0.007291\n",
      "Epoch 161, loss: 0.007231\n",
      "Epoch 162, loss: 0.007251\n",
      "Epoch 163, loss: 0.007090\n",
      "Epoch 164, loss: 0.007225\n",
      "Epoch 165, loss: 0.007117\n",
      "Epoch 166, loss: 0.007120\n",
      "Epoch 167, loss: 0.007122\n",
      "Epoch 168, loss: 0.007082\n",
      "Epoch 169, loss: 0.007289\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 170, loss: 0.007035\n",
      "Epoch 171, loss: 0.007306\n",
      "Epoch 172, loss: 0.007036\n",
      "Epoch 173, loss: 0.007174\n",
      "Epoch 174, loss: 0.007247\n",
      "Epoch 175, loss: 0.007096\n",
      "Epoch 176, loss: 0.007235\n",
      "Epoch 177, loss: 0.007189\n",
      "Epoch 178, loss: 0.007349\n",
      "Epoch 179, loss: 0.007238\n",
      "Epoch 180, loss: 0.007207\n",
      "Epoch 181, loss: 0.007203\n",
      "Epoch 182, loss: 0.007160\n",
      "Epoch 183, loss: 0.007089\n",
      "Epoch 184, loss: 0.007167\n",
      "Epoch 185, loss: 0.007191\n",
      "Epoch 186, loss: 0.007084\n",
      "Epoch 187, loss: 0.007116\n",
      "Epoch 188, loss: 0.007205\n",
      "Epoch 189, loss: 0.007100\n",
      "Epoch 190, loss: 0.007282\n",
      "Epoch 191, loss: 0.007278\n",
      "Epoch 192, loss: 0.007008\n",
      "Epoch 193, loss: 0.006936\n",
      "Epoch 194, loss: 0.007187\n",
      "Epoch 195, loss: 0.007254\n",
      "Epoch 196, loss: 0.007186\n",
      "Epoch 197, loss: 0.007182\n",
      "Epoch 198, loss: 0.007165\n",
      "Epoch 199, loss: 0.007190\n",
      "Epoch 0, loss: 0.007189\n",
      "Epoch 1, loss: 0.007106\n",
      "Epoch 2, loss: 0.007231\n",
      "Epoch 3, loss: 0.007295\n",
      "Epoch 4, loss: 0.007201\n",
      "Epoch 5, loss: 0.007177\n",
      "Epoch 6, loss: 0.007170\n",
      "Epoch 7, loss: 0.007137\n",
      "Epoch 8, loss: 0.007159\n",
      "Epoch 9, loss: 0.007321\n",
      "Epoch 10, loss: 0.007167\n",
      "Epoch 11, loss: 0.007330\n",
      "Epoch 12, loss: 0.007092\n",
      "Epoch 13, loss: 0.007157\n",
      "Epoch 14, loss: 0.007176\n",
      "Epoch 15, loss: 0.007256\n",
      "Epoch 16, loss: 0.007050\n",
      "Epoch 17, loss: 0.007207\n",
      "Epoch 18, loss: 0.007116\n",
      "Epoch 19, loss: 0.007254\n",
      "Epoch 20, loss: 0.007124\n",
      "Epoch 21, loss: 0.007147\n",
      "Epoch 22, loss: 0.007165\n",
      "Epoch 23, loss: 0.007267\n",
      "Epoch 24, loss: 0.007188\n",
      "Epoch 25, loss: 0.007097\n",
      "Epoch 26, loss: 0.007339\n",
      "Epoch 27, loss: 0.007143\n",
      "Epoch 28, loss: 0.007055\n",
      "Epoch 29, loss: 0.007196\n",
      "Epoch 30, loss: 0.007272\n",
      "Epoch 31, loss: 0.007062\n",
      "Epoch 32, loss: 0.007240\n",
      "Epoch 33, loss: 0.007152\n",
      "Epoch 34, loss: 0.007186\n",
      "Epoch 35, loss: 0.007131\n",
      "Epoch 36, loss: 0.007254\n",
      "Epoch 37, loss: 0.007147\n",
      "Epoch 38, loss: 0.007254\n",
      "Epoch 39, loss: 0.007257\n",
      "Epoch 40, loss: 0.007288\n",
      "Epoch 41, loss: 0.007354\n",
      "Epoch 42, loss: 0.007276\n",
      "Epoch 43, loss: 0.007239\n",
      "Epoch 44, loss: 0.007072\n",
      "Epoch 45, loss: 0.007177\n",
      "Epoch 46, loss: 0.007175\n",
      "Epoch 47, loss: 0.007071\n",
      "Epoch 48, loss: 0.007296\n",
      "Epoch 49, loss: 0.007202\n",
      "Epoch 50, loss: 0.007177\n",
      "Epoch 51, loss: 0.007049\n",
      "Epoch 52, loss: 0.007115\n",
      "Epoch 53, loss: 0.007061\n",
      "Epoch 54, loss: 0.007161\n",
      "Epoch 55, loss: 0.007117\n",
      "Epoch 56, loss: 0.007181\n",
      "Epoch 57, loss: 0.007129\n",
      "Epoch 58, loss: 0.007273\n",
      "Epoch 59, loss: 0.007143\n",
      "Epoch 60, loss: 0.007311\n",
      "Epoch 61, loss: 0.007259\n",
      "Epoch 62, loss: 0.007092\n",
      "Epoch 63, loss: 0.007196\n",
      "Epoch 64, loss: 0.007153\n",
      "Epoch 65, loss: 0.007085\n",
      "Epoch 66, loss: 0.007239\n",
      "Epoch 67, loss: 0.007270\n",
      "Epoch 68, loss: 0.007143\n",
      "Epoch 69, loss: 0.007295\n",
      "Epoch 70, loss: 0.007070\n",
      "Epoch 71, loss: 0.007132\n",
      "Epoch 72, loss: 0.007180\n",
      "Epoch 73, loss: 0.007154\n",
      "Epoch 74, loss: 0.007253\n",
      "Epoch 75, loss: 0.007137\n",
      "Epoch 76, loss: 0.007117\n",
      "Epoch 77, loss: 0.007165\n",
      "Epoch 78, loss: 0.007227\n",
      "Epoch 79, loss: 0.007213\n",
      "Epoch 80, loss: 0.007261\n",
      "Epoch 81, loss: 0.007261\n",
      "Epoch 82, loss: 0.007236\n",
      "Epoch 83, loss: 0.007034\n",
      "Epoch 84, loss: 0.007133\n",
      "Epoch 85, loss: 0.007090\n",
      "Epoch 86, loss: 0.007071\n",
      "Epoch 87, loss: 0.007274\n",
      "Epoch 88, loss: 0.007107\n",
      "Epoch 89, loss: 0.007235\n",
      "Epoch 90, loss: 0.007330\n",
      "Epoch 91, loss: 0.007173\n",
      "Epoch 92, loss: 0.007171\n",
      "Epoch 93, loss: 0.007328\n",
      "Epoch 94, loss: 0.007200\n",
      "Epoch 95, loss: 0.007105\n",
      "Epoch 96, loss: 0.007111\n",
      "Epoch 97, loss: 0.007143\n",
      "Epoch 98, loss: 0.007199\n",
      "Epoch 99, loss: 0.007137\n",
      "Epoch 100, loss: 0.007235\n",
      "Epoch 101, loss: 0.007275\n",
      "Epoch 102, loss: 0.007166\n",
      "Epoch 103, loss: 0.007244\n",
      "Epoch 104, loss: 0.007077\n",
      "Epoch 105, loss: 0.007044\n",
      "Epoch 106, loss: 0.007166\n",
      "Epoch 107, loss: 0.007157\n",
      "Epoch 108, loss: 0.007193\n",
      "Epoch 109, loss: 0.007010\n",
      "Epoch 110, loss: 0.007159\n",
      "Epoch 111, loss: 0.007095\n",
      "Epoch 112, loss: 0.007185\n",
      "Epoch 113, loss: 0.007220\n",
      "Epoch 114, loss: 0.007137\n",
      "Epoch 115, loss: 0.007095\n",
      "Epoch 116, loss: 0.007190\n",
      "Epoch 117, loss: 0.007147\n",
      "Epoch 118, loss: 0.007305\n",
      "Epoch 119, loss: 0.007051\n",
      "Epoch 120, loss: 0.007106\n",
      "Epoch 121, loss: 0.007048\n",
      "Epoch 122, loss: 0.007208\n",
      "Epoch 123, loss: 0.007235\n",
      "Epoch 124, loss: 0.007206\n",
      "Epoch 125, loss: 0.007120\n",
      "Epoch 126, loss: 0.007220\n",
      "Epoch 127, loss: 0.007224\n",
      "Epoch 128, loss: 0.007297\n",
      "Epoch 129, loss: 0.007237\n",
      "Epoch 130, loss: 0.007171\n",
      "Epoch 131, loss: 0.007196\n",
      "Epoch 132, loss: 0.007053\n",
      "Epoch 133, loss: 0.007107\n",
      "Epoch 134, loss: 0.007160\n",
      "Epoch 135, loss: 0.007287\n",
      "Epoch 136, loss: 0.007256\n",
      "Epoch 137, loss: 0.007139\n",
      "Epoch 138, loss: 0.007070\n",
      "Epoch 139, loss: 0.007196\n",
      "Epoch 140, loss: 0.007261\n",
      "Epoch 141, loss: 0.007213\n",
      "Epoch 142, loss: 0.007162\n",
      "Epoch 143, loss: 0.007198\n",
      "Epoch 144, loss: 0.007196\n",
      "Epoch 145, loss: 0.007146\n",
      "Epoch 146, loss: 0.007188\n",
      "Epoch 147, loss: 0.007270\n",
      "Epoch 148, loss: 0.007148\n",
      "Epoch 149, loss: 0.007122\n",
      "Epoch 150, loss: 0.007138\n",
      "Epoch 151, loss: 0.007185\n",
      "Epoch 152, loss: 0.007114\n",
      "Epoch 153, loss: 0.007055\n",
      "Epoch 154, loss: 0.006997\n",
      "Epoch 155, loss: 0.007187\n",
      "Epoch 156, loss: 0.007152\n",
      "Epoch 157, loss: 0.007083\n",
      "Epoch 158, loss: 0.007184\n",
      "Epoch 159, loss: 0.006997\n",
      "Epoch 160, loss: 0.007232\n",
      "Epoch 161, loss: 0.007084\n",
      "Epoch 162, loss: 0.007228\n",
      "Epoch 163, loss: 0.007089\n",
      "Epoch 164, loss: 0.007148\n",
      "Epoch 165, loss: 0.007115\n",
      "Epoch 166, loss: 0.007242\n",
      "Epoch 167, loss: 0.007031\n",
      "Epoch 168, loss: 0.007214\n",
      "Epoch 169, loss: 0.007191\n",
      "Epoch 170, loss: 0.007177\n",
      "Epoch 171, loss: 0.007217\n",
      "Epoch 172, loss: 0.007102\n",
      "Epoch 173, loss: 0.007183\n",
      "Epoch 174, loss: 0.007145\n",
      "Epoch 175, loss: 0.007415\n",
      "Epoch 176, loss: 0.007236\n",
      "Epoch 177, loss: 0.007192\n",
      "Epoch 178, loss: 0.007200\n",
      "Epoch 179, loss: 0.007069\n",
      "Epoch 180, loss: 0.007341\n",
      "Epoch 181, loss: 0.007327\n",
      "Epoch 182, loss: 0.006992\n",
      "Epoch 183, loss: 0.007108\n",
      "Epoch 184, loss: 0.007119\n",
      "Epoch 185, loss: 0.007133\n",
      "Epoch 186, loss: 0.007190\n",
      "Epoch 187, loss: 0.007184\n",
      "Epoch 188, loss: 0.007037\n",
      "Epoch 189, loss: 0.007082\n",
      "Epoch 190, loss: 0.007196\n",
      "Epoch 191, loss: 0.007242\n",
      "Epoch 192, loss: 0.007115\n",
      "Epoch 193, loss: 0.007181\n",
      "Epoch 194, loss: 0.007300\n",
      "Epoch 195, loss: 0.007176\n",
      "Epoch 196, loss: 0.007171\n",
      "Epoch 197, loss: 0.007137\n",
      "Epoch 198, loss: 0.007125\n",
      "Epoch 199, loss: 0.007305\n",
      "Epoch 0, loss: 0.007091\n",
      "Epoch 1, loss: 0.007186\n",
      "Epoch 2, loss: 0.007321\n",
      "Epoch 3, loss: 0.007200\n",
      "Epoch 4, loss: 0.007332\n",
      "Epoch 5, loss: 0.007158\n",
      "Epoch 6, loss: 0.007248\n",
      "Epoch 7, loss: 0.007123\n",
      "Epoch 8, loss: 0.007227\n",
      "Epoch 9, loss: 0.007152\n",
      "Epoch 10, loss: 0.007121\n",
      "Epoch 11, loss: 0.007156\n",
      "Epoch 12, loss: 0.007216\n",
      "Epoch 13, loss: 0.007316\n",
      "Epoch 14, loss: 0.007379\n",
      "Epoch 15, loss: 0.007295\n",
      "Epoch 16, loss: 0.007045\n",
      "Epoch 17, loss: 0.007118\n",
      "Epoch 18, loss: 0.007069\n",
      "Epoch 19, loss: 0.007105\n",
      "Epoch 20, loss: 0.007159\n",
      "Epoch 21, loss: 0.007174\n",
      "Epoch 22, loss: 0.007161\n",
      "Epoch 23, loss: 0.007267\n",
      "Epoch 24, loss: 0.007277\n",
      "Epoch 25, loss: 0.007199\n",
      "Epoch 26, loss: 0.007039\n",
      "Epoch 27, loss: 0.007141\n",
      "Epoch 28, loss: 0.007282\n",
      "Epoch 29, loss: 0.007109\n",
      "Epoch 30, loss: 0.007271\n",
      "Epoch 31, loss: 0.007137\n",
      "Epoch 32, loss: 0.007023\n",
      "Epoch 33, loss: 0.007149\n",
      "Epoch 34, loss: 0.007069\n",
      "Epoch 35, loss: 0.007122\n",
      "Epoch 36, loss: 0.007194\n",
      "Epoch 37, loss: 0.007269\n",
      "Epoch 38, loss: 0.007195\n",
      "Epoch 39, loss: 0.007119\n",
      "Epoch 40, loss: 0.007236\n",
      "Epoch 41, loss: 0.007165\n",
      "Epoch 42, loss: 0.007137\n",
      "Epoch 43, loss: 0.007329\n",
      "Epoch 44, loss: 0.007156\n",
      "Epoch 45, loss: 0.007352\n",
      "Epoch 46, loss: 0.007199\n",
      "Epoch 47, loss: 0.007116\n",
      "Epoch 48, loss: 0.007263\n",
      "Epoch 49, loss: 0.007190\n",
      "Epoch 50, loss: 0.007212\n",
      "Epoch 51, loss: 0.007136\n",
      "Epoch 52, loss: 0.007154\n",
      "Epoch 53, loss: 0.007267\n",
      "Epoch 54, loss: 0.007230\n",
      "Epoch 55, loss: 0.007212\n",
      "Epoch 56, loss: 0.007166\n",
      "Epoch 57, loss: 0.007170\n",
      "Epoch 58, loss: 0.007257\n",
      "Epoch 59, loss: 0.007020\n",
      "Epoch 60, loss: 0.007242\n",
      "Epoch 61, loss: 0.007164\n",
      "Epoch 62, loss: 0.007191\n",
      "Epoch 63, loss: 0.007184\n",
      "Epoch 64, loss: 0.007159\n",
      "Epoch 65, loss: 0.007144\n",
      "Epoch 66, loss: 0.007123\n",
      "Epoch 67, loss: 0.007061\n",
      "Epoch 68, loss: 0.007198\n",
      "Epoch 69, loss: 0.007100\n",
      "Epoch 70, loss: 0.007183\n",
      "Epoch 71, loss: 0.007220\n",
      "Epoch 72, loss: 0.007126\n",
      "Epoch 73, loss: 0.007246\n",
      "Epoch 74, loss: 0.007108\n",
      "Epoch 75, loss: 0.007075\n",
      "Epoch 76, loss: 0.007203\n",
      "Epoch 77, loss: 0.007178\n",
      "Epoch 78, loss: 0.007160\n",
      "Epoch 79, loss: 0.007224\n",
      "Epoch 80, loss: 0.007020\n",
      "Epoch 81, loss: 0.007279\n",
      "Epoch 82, loss: 0.007205\n",
      "Epoch 83, loss: 0.007228\n",
      "Epoch 84, loss: 0.007175\n",
      "Epoch 85, loss: 0.007190\n",
      "Epoch 86, loss: 0.007128\n",
      "Epoch 87, loss: 0.007144\n",
      "Epoch 88, loss: 0.007128\n",
      "Epoch 89, loss: 0.007134\n",
      "Epoch 90, loss: 0.007233\n",
      "Epoch 91, loss: 0.007095\n",
      "Epoch 92, loss: 0.007174\n",
      "Epoch 93, loss: 0.007180\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 94, loss: 0.007242\n",
      "Epoch 95, loss: 0.007213\n",
      "Epoch 96, loss: 0.007209\n",
      "Epoch 97, loss: 0.007233\n",
      "Epoch 98, loss: 0.007138\n",
      "Epoch 99, loss: 0.006976\n",
      "Epoch 100, loss: 0.007209\n",
      "Epoch 101, loss: 0.007167\n",
      "Epoch 102, loss: 0.007133\n",
      "Epoch 103, loss: 0.007097\n",
      "Epoch 104, loss: 0.007064\n",
      "Epoch 105, loss: 0.007206\n",
      "Epoch 106, loss: 0.007296\n",
      "Epoch 107, loss: 0.007183\n",
      "Epoch 108, loss: 0.007074\n",
      "Epoch 109, loss: 0.007159\n",
      "Epoch 110, loss: 0.007267\n",
      "Epoch 111, loss: 0.007314\n",
      "Epoch 112, loss: 0.007185\n",
      "Epoch 113, loss: 0.007237\n",
      "Epoch 114, loss: 0.006930\n",
      "Epoch 115, loss: 0.007030\n",
      "Epoch 116, loss: 0.007225\n",
      "Epoch 117, loss: 0.007120\n",
      "Epoch 118, loss: 0.007276\n",
      "Epoch 119, loss: 0.007212\n",
      "Epoch 120, loss: 0.007232\n",
      "Epoch 121, loss: 0.007131\n",
      "Epoch 122, loss: 0.007147\n",
      "Epoch 123, loss: 0.007201\n",
      "Epoch 124, loss: 0.007209\n",
      "Epoch 125, loss: 0.007232\n",
      "Epoch 126, loss: 0.007322\n",
      "Epoch 127, loss: 0.007262\n",
      "Epoch 128, loss: 0.007276\n",
      "Epoch 129, loss: 0.007167\n",
      "Epoch 130, loss: 0.007219\n",
      "Epoch 131, loss: 0.007147\n",
      "Epoch 132, loss: 0.007097\n",
      "Epoch 133, loss: 0.007230\n",
      "Epoch 134, loss: 0.007352\n",
      "Epoch 135, loss: 0.007090\n",
      "Epoch 136, loss: 0.007172\n",
      "Epoch 137, loss: 0.007134\n",
      "Epoch 138, loss: 0.007322\n",
      "Epoch 139, loss: 0.007142\n",
      "Epoch 140, loss: 0.007095\n",
      "Epoch 141, loss: 0.007164\n",
      "Epoch 142, loss: 0.007232\n",
      "Epoch 143, loss: 0.007141\n",
      "Epoch 144, loss: 0.007188\n",
      "Epoch 145, loss: 0.007256\n",
      "Epoch 146, loss: 0.007187\n",
      "Epoch 147, loss: 0.007122\n",
      "Epoch 148, loss: 0.007314\n",
      "Epoch 149, loss: 0.007077\n",
      "Epoch 150, loss: 0.007385\n",
      "Epoch 151, loss: 0.007297\n",
      "Epoch 152, loss: 0.007188\n",
      "Epoch 153, loss: 0.007289\n",
      "Epoch 154, loss: 0.007155\n",
      "Epoch 155, loss: 0.007299\n",
      "Epoch 156, loss: 0.006974\n",
      "Epoch 157, loss: 0.007008\n",
      "Epoch 158, loss: 0.007215\n",
      "Epoch 159, loss: 0.007327\n",
      "Epoch 160, loss: 0.007215\n",
      "Epoch 161, loss: 0.007133\n",
      "Epoch 162, loss: 0.007302\n",
      "Epoch 163, loss: 0.007129\n",
      "Epoch 164, loss: 0.007086\n",
      "Epoch 165, loss: 0.006991\n",
      "Epoch 166, loss: 0.007123\n",
      "Epoch 167, loss: 0.007071\n",
      "Epoch 168, loss: 0.007416\n",
      "Epoch 169, loss: 0.007257\n",
      "Epoch 170, loss: 0.007215\n",
      "Epoch 171, loss: 0.007283\n",
      "Epoch 172, loss: 0.007304\n",
      "Epoch 173, loss: 0.007185\n",
      "Epoch 174, loss: 0.007189\n",
      "Epoch 175, loss: 0.007107\n",
      "Epoch 176, loss: 0.007096\n",
      "Epoch 177, loss: 0.007174\n",
      "Epoch 178, loss: 0.007226\n",
      "Epoch 179, loss: 0.007151\n",
      "Epoch 180, loss: 0.006954\n",
      "Epoch 181, loss: 0.007239\n",
      "Epoch 182, loss: 0.007255\n",
      "Epoch 183, loss: 0.007259\n",
      "Epoch 184, loss: 0.007146\n",
      "Epoch 185, loss: 0.007166\n",
      "Epoch 186, loss: 0.007167\n",
      "Epoch 187, loss: 0.007144\n",
      "Epoch 188, loss: 0.007158\n",
      "Epoch 189, loss: 0.007180\n",
      "Epoch 190, loss: 0.007235\n",
      "Epoch 191, loss: 0.007136\n",
      "Epoch 192, loss: 0.007179\n",
      "Epoch 193, loss: 0.007095\n",
      "Epoch 194, loss: 0.007185\n",
      "Epoch 195, loss: 0.007181\n",
      "Epoch 196, loss: 0.007134\n",
      "Epoch 197, loss: 0.007152\n",
      "Epoch 198, loss: 0.007038\n",
      "Epoch 199, loss: 0.007220\n",
      "Epoch 0, loss: 0.007102\n",
      "Epoch 1, loss: 0.007339\n",
      "Epoch 2, loss: 0.007007\n",
      "Epoch 3, loss: 0.007089\n",
      "Epoch 4, loss: 0.007113\n",
      "Epoch 5, loss: 0.007223\n",
      "Epoch 6, loss: 0.007148\n",
      "Epoch 7, loss: 0.007269\n",
      "Epoch 8, loss: 0.007236\n",
      "Epoch 9, loss: 0.007188\n",
      "Epoch 10, loss: 0.006956\n",
      "Epoch 11, loss: 0.007227\n",
      "Epoch 12, loss: 0.007045\n",
      "Epoch 13, loss: 0.007114\n",
      "Epoch 14, loss: 0.007170\n",
      "Epoch 15, loss: 0.007037\n",
      "Epoch 16, loss: 0.007141\n",
      "Epoch 17, loss: 0.006952\n",
      "Epoch 18, loss: 0.007132\n",
      "Epoch 19, loss: 0.007134\n",
      "Epoch 20, loss: 0.007350\n",
      "Epoch 21, loss: 0.007223\n",
      "Epoch 22, loss: 0.007182\n",
      "Epoch 23, loss: 0.007118\n",
      "Epoch 24, loss: 0.007208\n",
      "Epoch 25, loss: 0.007266\n",
      "Epoch 26, loss: 0.007133\n",
      "Epoch 27, loss: 0.007042\n",
      "Epoch 28, loss: 0.007204\n",
      "Epoch 29, loss: 0.007127\n",
      "Epoch 30, loss: 0.007210\n",
      "Epoch 31, loss: 0.007073\n",
      "Epoch 32, loss: 0.007204\n",
      "Epoch 33, loss: 0.007076\n",
      "Epoch 34, loss: 0.007255\n",
      "Epoch 35, loss: 0.007097\n",
      "Epoch 36, loss: 0.007244\n",
      "Epoch 37, loss: 0.007116\n",
      "Epoch 38, loss: 0.007154\n",
      "Epoch 39, loss: 0.007315\n",
      "Epoch 40, loss: 0.007020\n",
      "Epoch 41, loss: 0.007038\n",
      "Epoch 42, loss: 0.007227\n",
      "Epoch 43, loss: 0.007183\n",
      "Epoch 44, loss: 0.007197\n",
      "Epoch 45, loss: 0.007015\n",
      "Epoch 46, loss: 0.007261\n",
      "Epoch 47, loss: 0.007093\n",
      "Epoch 48, loss: 0.007042\n",
      "Epoch 49, loss: 0.007172\n",
      "Epoch 50, loss: 0.007162\n",
      "Epoch 51, loss: 0.007075\n",
      "Epoch 52, loss: 0.007169\n",
      "Epoch 53, loss: 0.007142\n",
      "Epoch 54, loss: 0.007244\n",
      "Epoch 55, loss: 0.007061\n",
      "Epoch 56, loss: 0.007227\n",
      "Epoch 57, loss: 0.007040\n",
      "Epoch 58, loss: 0.007257\n",
      "Epoch 59, loss: 0.007336\n",
      "Epoch 60, loss: 0.007193\n",
      "Epoch 61, loss: 0.007229\n",
      "Epoch 62, loss: 0.007013\n",
      "Epoch 63, loss: 0.007129\n",
      "Epoch 64, loss: 0.007231\n",
      "Epoch 65, loss: 0.007311\n",
      "Epoch 66, loss: 0.007067\n",
      "Epoch 67, loss: 0.007284\n",
      "Epoch 68, loss: 0.007267\n",
      "Epoch 69, loss: 0.007226\n",
      "Epoch 70, loss: 0.007259\n",
      "Epoch 71, loss: 0.007227\n",
      "Epoch 72, loss: 0.007199\n",
      "Epoch 73, loss: 0.007202\n",
      "Epoch 74, loss: 0.007115\n",
      "Epoch 75, loss: 0.007174\n",
      "Epoch 76, loss: 0.006961\n",
      "Epoch 77, loss: 0.007194\n",
      "Epoch 78, loss: 0.007199\n",
      "Epoch 79, loss: 0.007195\n",
      "Epoch 80, loss: 0.007246\n",
      "Epoch 81, loss: 0.007184\n",
      "Epoch 82, loss: 0.007222\n",
      "Epoch 83, loss: 0.007192\n",
      "Epoch 84, loss: 0.007222\n",
      "Epoch 85, loss: 0.007153\n",
      "Epoch 86, loss: 0.007166\n",
      "Epoch 87, loss: 0.007148\n",
      "Epoch 88, loss: 0.007194\n",
      "Epoch 89, loss: 0.007107\n",
      "Epoch 90, loss: 0.007188\n",
      "Epoch 91, loss: 0.007341\n",
      "Epoch 92, loss: 0.007098\n",
      "Epoch 93, loss: 0.007202\n",
      "Epoch 94, loss: 0.007170\n",
      "Epoch 95, loss: 0.007036\n",
      "Epoch 96, loss: 0.007116\n",
      "Epoch 97, loss: 0.007174\n",
      "Epoch 98, loss: 0.007116\n",
      "Epoch 99, loss: 0.007009\n",
      "Epoch 100, loss: 0.007078\n",
      "Epoch 101, loss: 0.007069\n",
      "Epoch 102, loss: 0.007204\n",
      "Epoch 103, loss: 0.007255\n",
      "Epoch 104, loss: 0.007112\n",
      "Epoch 105, loss: 0.006991\n",
      "Epoch 106, loss: 0.007073\n",
      "Epoch 107, loss: 0.007319\n",
      "Epoch 108, loss: 0.007175\n",
      "Epoch 109, loss: 0.007164\n",
      "Epoch 110, loss: 0.007141\n",
      "Epoch 111, loss: 0.007162\n",
      "Epoch 112, loss: 0.007146\n",
      "Epoch 113, loss: 0.007274\n",
      "Epoch 114, loss: 0.007158\n",
      "Epoch 115, loss: 0.007049\n",
      "Epoch 116, loss: 0.007197\n",
      "Epoch 117, loss: 0.007092\n",
      "Epoch 118, loss: 0.007210\n",
      "Epoch 119, loss: 0.007161\n",
      "Epoch 120, loss: 0.007130\n",
      "Epoch 121, loss: 0.007268\n",
      "Epoch 122, loss: 0.007165\n",
      "Epoch 123, loss: 0.007273\n",
      "Epoch 124, loss: 0.007110\n",
      "Epoch 125, loss: 0.007213\n",
      "Epoch 126, loss: 0.007237\n",
      "Epoch 127, loss: 0.007159\n",
      "Epoch 128, loss: 0.007194\n",
      "Epoch 129, loss: 0.007223\n",
      "Epoch 130, loss: 0.007070\n",
      "Epoch 131, loss: 0.007222\n",
      "Epoch 132, loss: 0.007005\n",
      "Epoch 133, loss: 0.007171\n",
      "Epoch 134, loss: 0.007172\n",
      "Epoch 135, loss: 0.007151\n",
      "Epoch 136, loss: 0.007158\n",
      "Epoch 137, loss: 0.007167\n",
      "Epoch 138, loss: 0.007185\n",
      "Epoch 139, loss: 0.007300\n",
      "Epoch 140, loss: 0.007154\n",
      "Epoch 141, loss: 0.007099\n",
      "Epoch 142, loss: 0.007145\n",
      "Epoch 143, loss: 0.007043\n",
      "Epoch 144, loss: 0.007214\n",
      "Epoch 145, loss: 0.007085\n",
      "Epoch 146, loss: 0.007090\n",
      "Epoch 147, loss: 0.007125\n"
     ]
    }
   ],
   "source": [
    "num_epochs = 200\n",
    "batch_size = 300\n",
    "\n",
    "learning_rates = [1e-3, 1e-4, 1e-5]\n",
    "reg_strengths = [1e-4, 1e-5, 1e-6]\n",
    "\n",
    "best_classifier = None\n",
    "best_val_accuracy = None\n",
    "\n",
    "# TODO use validation set to find the best hyperparameters\n",
    "# hint: for best results, you might need to try more values for learning rate and regularization strength \n",
    "# than provided initially\n",
    "\n",
    "for lr in learning_rates:\n",
    "    for rs in reg_strengths:\n",
    "        loss_history = classifier.fit(train_X, train_y, epochs=num_epochs, learning_rate=lr, batch_size=batch_size, reg=rs)\n",
    "        pred = classifier.predict(val_X)\n",
    "        accuracy = multiclass_accuracy(pred, val_y)\n",
    "        if best_val_accuracy is None or best_val_accuracy < accuracy:\n",
    "            best_classifier = classifier\n",
    "            best_val_accuracy = accuracy\n",
    "\n",
    "print('best validation accuracy achieved: %f' % best_val_accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Какой же точности мы добились на тестовых данных?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Linear softmax classifier test set accuracy: 0.204000\n"
     ]
    }
   ],
   "source": [
    "test_pred = best_classifier.predict(test_X)\n",
    "test_accuracy = multiclass_accuracy(test_pred, test_y)\n",
    "print('Linear softmax classifier test set accuracy: %f' % (test_accuracy, ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dlcourse",
   "language": "python",
   "name": "dlcourse"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
